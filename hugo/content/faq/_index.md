---
title: "Faq"
date: 2023-03-23T21:26:37Z
draft: false
type: faq
---

# Frequently Asked Questions

## dora.dev
### DevOps Quick Check
- What data are used to calculate my results?
- Quick Check results are based on data collected from respondents to the State of DevOps survey (the same survey on which the Accelerate State of DevOps Report is based). Your results are based on findings from the 2022 survey; you can also take the Quick Check based on results from [2019](/quickcheck/?year=2019) or [2021](/quickcheck/?year=2021). <small><em>(There was no Accelerate State of DevOps study in 2020.)</em></small>
- Why don't my results include a performance cluster?
- Prior versions of the Quick Check displayed results including a performance cluster (low, medium, high, elite). Because the number of clusters varies between studies (see below), and in order to focus on ongoing improvement at the level of individual teams, we no longer compute a performance cluster, but rather a percentile based on data from all research participants.

## Publications
### Accelerate State of DevOps 2022

- What is different in this year's State of DevOps Report?
- Comparing the 2022 results to the 2021 results, there are a number of findings that did not follow trends that we have seen consistently over the past several years. In fact, there were enough of these that we chose to add a section to the report—"Surprises"—summarizing them.
- _Why_ are things different?
- In a word: science. Fundamentally, our data collection methods are the same as they have been since the beginning of this research project, in 2014. But each year, there is a different (though likely overlapping) set of respondents, and we ask new or updated questions, and we perform different analyses. And of course, the world is always changing. We attempt to make sense of the data as we see it, and the data offered some surprises this year. In some cases, these major shifts reflect changes in the external reality of our respondents' practices and outcomes. In other cases, these may be artifacts of sampling and analysis methods. We cannot know for sure which is which, but we offer potential explanations that attempt to make sense of what the data tell us.
- Why are there only three clusters of performance this year?
- The analytic methods we use objectively reveal clusters of similar respondents within the set of responses. We have never dictated the number of clusters; they are emergent from the patterns in the data. In fact, before 2018, we consistently found three clusters (Low, Medium, High). From 2018 to 2021, there were four. This year, we again see three. The report speculates on why this is the case, but it is too soon to tell if it is a lasting effect.
- Why is this year's report focused so heavily on security?
- We dove deep on security in this year's research for two reasons:
    1. In the past two years, multiple high-profile, catastrophic security breaches (see: [SolarWinds](https://www.businessinsider.com/solarwinds-hack-explained-government-agencies-cyber-security-2020-12), [Codecov](https://www.reuters.com/technology/codecov-hackers-breached-hundreds-restricted-customer-sites-sources-2021-04-19/), and [Colonial Pipeline](https://www.bloomberg.com/news/articles/2021-06-04/hackers-breached-colonial-pipeline-using-compromised-password) attacks) have brought the dangers of a vulnerable software supply chain to the public's attention.
    2. Prior DORA research revealed the importance of incorporating software supply-chain security measures early, and throughout the development process. In 2021, for example, our data showed that elite performers excel in implementing security best practices, and that development teams that embrace security see significant value driven to the business.

Do you have a question that is not answered here? Discuss it with the [DORA community](https://dora.community)!